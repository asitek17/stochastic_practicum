\documentclass[11pt]{article}

\usepackage{a4wide}
\usepackage{amssymb}
\usepackage[T1,T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{amsthm}
\usepackage{placeins}

\newtheorem{theorem}{Теорема}[section]
\newtheorem{lemma}{Лемма}[section]
\newtheorem{statement}{Утвержение}[section]
\newtheorem{corollary}{Замечание}[theorem]
\newtheorem{definition}{Определение}[section]

\begin{document}

\thispagestyle{empty}

\begin{center}
\ \vspace{-3cm}

\includegraphics[width=0.5\textwidth]{msu.eps}\\
{\scshape Московский государственный университет имени М.~В.~Ломоносова}\\
Факультет вычислительной математики и кибернетики\\
Кафедра системного анализа

\vfill

{\LARGE Отчёт по Практикуму}

\vspace{1cm}

{\Huge\bfseries <<Стохастический анализ>>}
\end{center}

\vspace{1cm}

\begin{flushright}
  \large
  \textit{Студент 415 группы}\\
  И.В. Ковшов

  \vspace{5mm}

  \textit{Руководитель практикума}\\
  д.ф.-м.н., профессор С.\,Н.~Смирнов
\end{flushright}

\vfill

\begin{center}
Москва, 2024
\end{center}

\newpage

\section{Задание 1}

1. Реализовать генератор схемы Бернулли с заданной вероятностью успеха $p$. На
основе генератора схемы Бернулли построить датчик биномиального распределения.

2. Реализовать генератор геометрического распределения; проверить для данного распределения свойство отсутствия памяти.

3. Промоделировать игру в орлянку: бесконечную последовательность независимых испытаний Бернулли 
с бросанием “правильной” (честной, $p=0.5$) монеты. Величина “выигрыша” $S_n$ определяется как сумма по $n$ испытаниям значений 1 и -1 в зависимости от выпавшей стороны монеты. Проиллюстрировать в виде ломаной поведение нормированной суммы $Y(i)=\frac{S_i}{\sqrt{n}}$ как функцию от номера испытания $i$ для отдельно взятой траектории. Дать теоретическую оценку для значения $Y(n)$ при $n\to\infty$.

\subsection{Часть 1}

\begin{definition}
 Схемой Бернулли с заданной вероятностью успеха p называется эксперимент, состоящий из серии испытаний, удовлетворяющих следующим свойствам:
 \begin{enumerate}
  \item Отсутствие взаимного влияния.
  \item Воспроизводимость испытаний (испытания производятся в сходных условиях).
  \item В каждом испытании наблюдается признак, причем вероятность его ``успеха``, равна $p$.
 \end{enumerate}
 
\end{definition}

\begin{definition}
  Случайная величина X, называется Бернуллиевской случайной величиной, если:
\[
 X = 
  \begin{cases}
    1,~ \xi \in [0;p), \\
    0,~ \xi \in [p;1].
  \end{cases}
\]

Обозначение: $X \sim Bern(p)$.
\end{definition}

\begin{definition}
 Пусть $X_1 ,\ldots, X_n$ — набор независимых случайных величин с рас-
пределением Бернулли с параметром p. Тогда, случайная величина
\[
  Y = X_1 + \ldots + X_n
\]
называется случайной величиной, имеющей биномиальное распределение с параметрами
$n$ и $p$.
Обозначение: $Y \sim Bi(n, p)$.
\end{definition}

Функция вероятности в данном случае будет иметь вид:
\[
 P(Y = k) = C_n^k~ p^k (1-p)^{n-k}.
\]
Промоделирируем сравнение теоретического и экспериментального распределения.

\begin{figure}[ht]

    \includegraphics[width=0.7\linewidth]{1_1.eps} 
    \caption{Сравнение эмпирического и теоретического распределения вероятностей \\ 
    для $N=10^4$.}
\end{figure}    
\FloatBarrier


\subsection{Часть 2}
Геометрическое распределение может иметь две различные формулировки, укажем одно, которое будет использоваться далее.
\begin{definition}
 Случайная величина $X$ принадлежит геометрическому распределению, если: \\
 - её распределение вероятностей равно номеру первого успеха в испытании Бернулли. (принимает значения $n=1,2,3,\ldots$). \\
Обозначение: $X \sim Geom(p)$. 
\end{definition}

Соответственно имеем
$$
 n \geqslant 1,~ 0 \leqslant p \leqslant 1,~ q \equiv 1 -p.
$$
Ф-ия вер-ти в данном случае будет равна
$$
    P(\xi = k) = q^{k-1}p.
$$
Если моделировать это с помощью испытаний Бернулли, то получается будет $k-1$ неудача, а на $k$-ом номере будет успех. \

\begin{figure}[ht]

    \includegraphics[width=0.7\linewidth]{1_2(geom).eps} 
    \caption{Сравнение эмпирического и теоретического распределения вероятностей \\ 
    для $N=10^6$.}
\end{figure}  

Посчитаем вер-ть успеха на одном из первых $n+1$ испытаний. 
$$
    S_{n+1} = \sum_{i=1}^{n+1} p (1-p)^{i-1} = \text{ |сумма геометрической прогресии| }= 1 - (1-p)^{n+1}
$$
Теперь рассмотрим событие $A$ - номер первого успеха равен $n$, тогда мы можем интерпретировать его как нахождение случайной величины в промежутке $S_n$ и $S_{n+1}$.

$$
\begin{gathered}
\{\xi=n\}=
\left\{S_n < \alpha \leq S_{n+1}\right\}=
\left\{1-(1-p)^n < \alpha \leq 1-(1-p)^{n + 1}\right\}= \\
=\left\{(1-p)^{n+1} \leq 1-\alpha<(1-p)^n\right\}=\{(n+1) \ln (1-p) \leq \ln (1-\alpha)<n \ln (1-p)\}= \\ 
\left\{n<\frac{\ln (1-\alpha)}{\ln (1-p)} \leq n+1\right\}
\end{gathered}
$$

Получаем:
$$
\xi=\left\lfloor \frac{\ln (1 - \alpha)}{\ln (1-p)}\right\rfloor.
$$
Или же можно заменить $1 - \alpha$ на $\alpha$, потому что $\alpha \sim U[0,1]$:
$$
\xi=\left\lfloor \frac{\ln (\alpha)}{\ln (1-p)}\right\rfloor.
$$

\begin{theorem}
  Если $Z \sim \operatorname{Geom}(p)$, то

$$
\mathbb{P}(Z>m+n \mid Z \geqslant m)=\mathbb{P}(Z>n)
$$
для любых цельх неотрицательных $n$ и $m$.
\begin{proof}
 $$
\begin{aligned}
\mathbb{P}(Z>m+n \mid Z \geqslant m)=\frac{\mathbb{P}(Z>m+n, Z \geqslant m)}{\mathbb{P}(Z \geqslant m)}= \\
\frac{\mathbb{P}(Z>m+n)}{\mathbb{P}(Z \geqslant m)}=\frac{(1-p)^{m+n+1}}{(1-p)^m}=(1-p)^{n+1}=\mathbb{P}(Z>n) .
\end{aligned}
$$

\end{proof}
\end{theorem}
Данная теорема показывает, что геометрическое распределение обладает свойством отсутствия памяти.\\
Промоделирируем это на примере, где для наглядности сразу сдвинем условную выборку на значение $m$(в теореме) или  ($shift$ на графике).
\begin{figure}[h]

    \includegraphics[width=1\linewidth]{1_2.eps} 
    \caption{Проверка свойства отсутствия памяти \\ 
    для геометрического распределения и параметров $N=10^6,~ p=0.3$.}
\end{figure}  
\FloatBarrier


\subsection{Часть 3}

Случайную величину из распределения для единичной игры в орлянку буду обозначать $Eagle(p)$,\
где p - вероятность "успеха". \
Математическое ожидание и дисперсия для единичной игры в орлянку.
$$
\begin{gathered}
    p = 0.5, \psi \sim Eagle(p), \\
E \psi = \frac{1}{2} * 1 + \frac{1}{2} * (-1) = 0. \\
E \psi^2 = \frac{1}{2} * (1)^2 + \frac{1}{2} * (-1)^2 = 1, \\
D \psi = E \psi^2 - (E \psi)^2 = 1 - 0 = 1.
\end{gathered}
$$
Промоделирируем это траекторию нормированной суммы
\begin{figure}[ht]

    \includegraphics[width=1\linewidth]{1_3.eps} 
    \caption{Игра в "орлянку" с параметрами $num~series=1000$.}
\end{figure}  

\begin{theorem}[Центральная Предельная Теорема]
 Пусть $X_1,X_2,\ldots,X_n,\ldots$ – бесконечная последовательность н.о.р.с.в., определённых на одном вероятностном пространстве и имеющих конечное математическое ожидание ($\mu$) и дисперсию ($\sigma^2$). Рассмотрим $S_n = \sum_{i=1}^n X_i $. \\
 Тогда:
$$
 \frac{S_n - n \mu_n}{\sigma \sqrt{n}} \underset{t \to +\infty}{\overset{d(\cdot)}{\rightarrow}} N(0, 1).
$$
  Доказывалась теорема в \cite{2}.
\end{theorem}

Соответственно, используя центральную предельную теорему получаем
$$
    Y(n) \sim N(0, 1).
$$


\section{Задание 2}

1. Построить датчик сингулярного распределения, имеющий в качестве функции распределения канторову лестницу. 
С помощью критерия Колмогорова убедиться в корректности работы датчика.

2.  Для канторовых случайных величин с помощью критерия Смирнова проверить
свойство симметричности относительно $\frac{1}{2}$ ($X$ и $1 - X$ распределены одинаково)
и свойство самоподобия относительно деления на $3$ (условное распределение $Y$
при условии $Y \in [0, 1/3]$ совпадает с распределением $\frac{Y}{3}$).

3. Рассчитать значения математического ожидания и дисперсии для данного распределения. Сравнить теоретические значения с эмпирическими (для различных объемов выборок), проиллюстрировать сходимость эмпирических значений к теоретическим.

\subsection{Часть 1}
Под канторовой лестницей понимается непрерывная монотонная функция \\
$F:[0,1] \rightarrow$ $[0,1]$, построенная следующим образом:

 \begin{enumerate}
  \item $F(0)=0, F(1)=1$,
  \item Разбиваем отрезок $[0,1]$ на три равные части. Внутри центральной части полагаем $F(x)=\frac{1}{2}, x \in\left[\frac{1}{3}, \frac{2}{3}\right]$.
  \item Два оставшихся сегмента снова разбиваем на три равные части, полагая в центральных сегментах $F(x)=\frac{1}{4}$ и $\frac{3}{4}$.
  \item Продолжаем данный процесс до бесконечности, определяя на внутренних сегментах $F(x)$ равной среднему арифметическому из соседних определенных значений. На остальных точках отрезка $[0,1]$ определяем $F(x)$ по непрерывности.
 \end{enumerate}

Промоделирируем график лестницы Кантора с помощью рекурсивного определения:
\begin{figure}[ht]

    \includegraphics[width=1\linewidth]{2_1.eps} 
    \caption{График лестницы Кантора}
\end{figure}  

\begin{figure}[ht]

    \includegraphics[width=1\linewidth]{2_1(sample).eps} 
    \caption{График выборки из распределения}
\end{figure}  
\FloatBarrier
 
Для моделирования выборки воспользуемся альтернативным определением. \\
Для моделирования будем использовать двоичную и троичную запись числа.

Будем брать число $x \in [0;1]$ и будем переводить его в троичную систему счисления.\
Для получения значений $F(x)$ будем отбрасывать все цифры после первой единицы в записи числа, и потом заменить в получившемся числе все $2$ на $1$, таким образом получим представление числа в двоичной системе счисления. Точки из самого Канторово множества 
представление числа, в которых нет цифры $1$.\
Воспользуемся генератором схемы Бернулли для моделирования. Тогда, умножив на 2 вектор случайных величин из схемы Бернулли получим число в троичной записи, останется конвертировать его обратно в обычную ($10$-ю) систему счисления. Необходимо ввести дополнительный параметр $\epsilon$, как степень точности получаемого числа.\
Проведём оценку:
$$
\sum_{n=N+1}^{+\infty} \frac{2 * \eta_n}{3^n} \leq \sum_{n=N+1}^{+\infty} \frac{2}{3^n}=2 * \frac{1}{(1 - \frac{1}{3}) * 3^{N+1}}=\frac{1}{3^N} \leq \epsilon
$$

Таким образом, можем получить длину дробной части числа, достаточную для получения заданной точности:
$$
N \geq \frac{\ln \frac{1}{\varepsilon}}{\ln 3}
$$

Значение функции распределения можем посчитать по формуле:
$$
F(x)=\sum_{n=1}^N \frac{\eta_n}{2^n}, \quad x=\sum_{n=1}^N \frac{2 \cdot \eta_n}{3^n}
$$

 Критерий согласия Колмогорова.\\
Критерий согласия Колмогорова предназначен для проверки гипотезы о принадлежности выборки некоторому закону распределения. \
Гипотеза $ H_0 : F_n(x) \sim F(x) $. \
Статистика критерия для эмпирической функции распределения $F_n(x)$ определяется следующим образом:
\[
 D_n = \sup\limits_{x \in \mathbb{R}} | F_n(x) - F(x) |.
\]
Распределение статистики Колмогорова. \\
Тогда по теореме Колмогорова для введённой статистики справедливо:
\[
 \forall t > 0: \lim\limits_{n \to + \infty} \mathbb{P}(\sqrt{n}D_n \leqslant t) = K(t) = \sum\limits_{j = - \infty}^{+\infty} (-1)^{j} e^{-2j^2t^2} = 2 \sum\limits_{j = 1}^{+\infty} (-1){j} e^{-2j^2t^2} + 1 =  
\]
Принятие решения по Колмогорову
Если статистика $\sqrt{n} D_n$ превышает процентную точку распределения Колмогорова $K_{\alpha}$ заданного уровня значимости $\alpha$, то нулевая гипотеза $H_0$ (о соответствии закону $F(x)$) отвергается. Иначе гипотеза принимается на уровне $\alpha$.


\begin{tabular}{|c|c|c|}
\hline
\multicolumn{3}{|c|}{Проверка корректности датчика с помощью критерия Колмогорова} \\
\hline
Число испытаний & Уровень значимости & Частота принятия гипотезы \\
\hline
100 & 2\% & 0.98 \\
1000 & 2\% & 0.992 \\
100 & 10\% & 0.94 \\
1000 & 10\% & 0.957 \\
\hline
\end{tabular}

\subsection{Часть 2}

Критерий Смирнова. \\
Критерий однородности Смирнова используется для проверки гипотезы о принадлежности двух независимых выборок одному закону распределения. 
$$
H_0 : F_{1}(x) \sim F_{2}(x),
$$
где $F_1(x), F_2(x)$ - два эмпирических распределения, на выборках объёмов $n$ и $m$ соответственно.

Статистика критерия для эмпирической функции распределения $F_1(x)$, $F_2(x)$ определяется следующим образом:
\[
 D_{n,m} = \sup\limits_{x \in \mathbb{R}} |F_1(x) - F_2(x)|.
\]
Распределение статистики Смирнова. \\
Для введённой статистики справедливо:
\[
 \forall t > 0: \lim\limits_{n \to + \infty} \mathbb{P}(\sqrt{\frac{nm}{n + m}}D_{n,m} \leqslant t) = K(t) = \sum\limits_{j = - \infty}^{+\infty} (-1){j} e^{-2j^2t^2} = 2 \sum\limits_{j = 1}^{+\infty} (-1)^{j} e^{-2j^2t^2} + 1 =  
\]
Принятие решения по Колмогорову
Если статистика $\frac{nm}{n + m}D_{n,m}$ превышает процентную точку распределения Колмогорова $K_{\alpha}$ заданного уровня значимости $\alpha$, то нулевая гипотеза $H_0$ (о соответствии закону $F(x)$) отвергается. Иначе гипотеза принимается на уровне $\alpha$.



\begin{tabular}{|c|c|c|}
\hline
\multicolumn{3}{|c|}{Проверка свойства симметричности относительно $0.5$ с помощью критерия Смирнова} \\
\hline
Число испытаний & Уровень значимости & Частота принятия гипотезы \\
\hline
100 & 5\% & 0.97 \\
1000 & 5\% & 0.955 \\
100 & 10\% & 0.86 \\
1000 & 10\% & 0.91 \\
\hline
\end{tabular}

\begin{tabular}{|c|c|c|}
\hline
\multicolumn{3}{|c|}{Проверка свойства самоподобия относительно деления на 3 с помощью критерия Смирнова} \\
\hline
Число испытаний & Уровень значимости & Частота принятия гипотезы \\
\hline
100 & 5\% & 0.95 \\
1000 & 5\% & 0.94 \\
100 & 10\% & 0.91 \\
1000 & 10\% & 0.887 \\
\hline
\end{tabular}


\subsection{Часть 3}
Для рассчёта математического ожидания воспользуемся доказанным ранее свойством: \\
симметричностью относительно $\frac{1}{2}$. \\
Критерий Смирнова проверяет две выборки на принадлежность одному закону распределения, соответственно получаем
$$
    E~[X] = E~[1 - X] = 1 - E~X, \\
    E~X = \frac{1}{2}.
$$

Для нахождения дисперсии воспользуемся альтернативное определение для лестницы Кантора, использовавшееся ранее. Учтём, что используется схема Бернулли с вероятностью успеха $p=0.5$, соответственно математическое ожидание случайной величины будет равно $0 * 0.5 + 1 * 0.5 = 0.5$.
$$
    D~[X] = \sum_{i=1}^{+\infty} (\frac{1}{3^{2n}}) =  \sum_{i=1}^{+\infty} (\frac{1}{9^{n}}) = \frac{\frac{1}{9}}{1 - \frac{1}{9}} = \frac{1}{8}.
$$

\begin{figure}[ht]

    \includegraphics[width=1\linewidth]{2_3(mean).eps} 
    \caption{Сходимость среднего}
\end{figure}  


\begin{figure}[ht]

    \includegraphics[width=1\linewidth]{2_3(std).eps} 
    \caption{Сходимость дисперсии}
\end{figure} 

\FloatBarrier

\section{Задание 3}
\begin{enumerate}
 \item Построить датчик экспоненциального распределения. Проверить для данного распределения свойство остутствия памяти.
 \item Пусть $X_1,\ldots,X_n$ — независимые экспоненциально распределенные случайные
величины с параметрами $\lambda_1 ,\ldots,\lambda_n$. Найти распределение случайной величины
$Y = \min(X_1 ,\ldots,X_n)$.
  \item На основе датчика экспоненциального распределения построить датчик пуассоновского распределения.
  \item Построить датчик пуассоновского распределения как предел биномиального распределения. Убедиться в корректности построенного датчика при помощи критерия $\chi^2$ Пирсона.
  \item Построить датчик стандартного нормального распределения методом моделирования случайных величин парами с переходом в полярные координаты (преобразование Бокса-Мюллера). Проверить при помощи $t$-критерия Стьюдента равен-
ство математических ожиданий, а при помощи критерия Фишера — равенство дисперсий.

\end{enumerate}

\subsection{3.1}
Случайная величина $X$ имеет экспоненциальное распределение с параметром $\lambda > 0$, если её функция плотности вероятности задана следующим образом:
$$
f_X(x) = 
\begin{cases} 
\lambda e^{-\lambda x}, & \text{если } x \geq 0, \\ 
0, & \text{иначе.} 
\end{cases}
$$


Функция распределения $F_X(x)$ для экспоненциальной случайной величины определяется как:
$$
F_X(x) = P(X \leq x) = 1 - e^{-\lambda x}, \quad x \geq 0
$$


Чтобы использовать метод обратного преобразования, необходимо найти обратную функцию к $F_X(x)$. Решим уравнение:

$$
u = F_X(x) = 1 - e^{-\lambda x}
$$

Решая относительно $x$, получаем:

$$
e^{-\lambda x} = 1 - u \implies -\lambda x = \ln(1 - u) \implies x = -\frac{1}{\lambda} \ln(1 - u)
$$

Таким образом, обратная функция имеет вид:

$$
F_X^{-1}(u) = -\frac{1}{\lambda} \ln(1 - u)
$$

$$X \sim Exp(\lambda)$$
Если случайная величина $X$ имеет экспоненциальное распределение с параметром $\lambda$, то согласно полученному ранее можно использовать выражение
$$
    X = - \frac{1}{\lambda} \ln{(1 - U)},
$$
где $U \sim [0;1]$ - равномерное распределние (можно заменить $1-U$ на $U$). \\
Или же вместо равномерного распределения можно воспользоваться псевдослучайными числами.


\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{3_1(distr).eps} 
    \caption{Графики плотностей распределения}
\end{figure}  


\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{3_1(memory).eps} 
    \caption{Проверка свойства отсутствия памяти}
\end{figure}  
\FloatBarrier
\subsection{Часть 3.2}

Пусть $X_1, X_2, \ldots, X_n$ — независимые экспоненциально распределенные случайные величины с параметрами $\lambda_1, \lambda_2, \ldots, \lambda_n$. Найдем распределение случайной величины $Y = \min(X_1, X_2, \ldots, X_n)$.

Сначала найдем функцию распределения для $Y$:

$$
F_Y(y) = P(Y \leq y) = P(\min(X_1, X_2, \ldots, X_n) \leq y).
$$


Это событие происходит, если каждая из случайных величин $X_i$ меньше или равна $y$. Таким образом:

$$
F_Y(y) = 1 - P(Y > y) = 1 - P(X_1 > y, X_2 > y, \ldots, X_n > y).
$$


Для экспоненциально распределенной случайной величины с параметром $\lambda_i$, вероятность того, что $X_i > y$, равна:

$$
P(X_i > y) = e^{-\lambda_i y}.
$$


Поскольку $X_1, X_2, \ldots, X_n$ независимы:

$$
P(X_1 > y, X_2 > y, \ldots, X_n > y) = P(X_1 > y) P(X_2 > y) \ldots P(X_n > y).
$$


Таким образом:

$$
P(X_1 > y, X_2 > y, \ldots, X_n > y) = e^{-\lambda_1 y} e^{-\lambda_2 y} \ldots e^{-\lambda_n y} = e^{-(\lambda_1 + \lambda_2 + \ldots + \lambda_n)y}.
$$


Теперь подставим это выражение обратно в функцию распределения $F_Y(y)$:

$$
F_Y(y) = 1 - P(Y > y) = 1 - e^{-(\lambda_1 + \lambda_2 + \ldots + \lambda_n)y}.
$$


Чтобы найти функцию плотности вероятности $f_Y(y)$, необходимо продифференцировать функцию распределения:

$$
f_Y(y) = \frac{d}{dy} F_Y(y) = (\lambda_1 + \lambda_2 + \ldots + \lambda_n)e^{-(\lambda_1 + \lambda_2 + \ldots + \lambda_n)y}.
$$


Таким образом, случайная величина $Y = \min(X_1, X_2, ..., X_n)$ также имеет экспоненциальное распределение с параметром:

$$
\lambda_Y = \lambda_1 + \lambda_2 + ... + \lambda_n.
$$

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{3_2.eps} 
    \caption{Сравнение двух методов построенная: на основе утверждения и поиск минимума по выборке}
\end{figure} 
\FloatBarrier

\subsection{Часть 3.3}


\begin{lemma}
 

Если $E_1, E_2, \ldots$ - i.i.d(независимые одинаково распределённые с.в.) экспоненциально распределённые случайные величины, и $X$ это наименьшее число, такое что:
$$
\sum_{i=1}^{X+1} E_i> 1
$$
тогда $X \sim Pois(\lambda)$.

\begin{proof}
 
Пусть $f_k$ - плотность $(k)$-й случайной величины. Тогда,
$$
P(X \leq k)=P\left(\sum_{i=1}^{k+1} E_i> 1 \right)=\int_\lambda^{\infty} f_{k+1}(y) d y .
$$

Если $ E_1, E_2, \ldots, E_k $ — независимые экспоненциальные случайные величины с параметром $ \lambda $, то сумма:
$$
S_k = \sum_{i=1}^k E_i
$$
имеет гамма-распределение с параметрами $k$ и $\lambda$. Плотность вероятности этого распределения записывается как:
$$
f_k(y) = \frac{\lambda^k y^{k-1} e^{-\lambda y}}{(k-1)!}, \quad y > 0.
$$
Пользуясь этим, получаем
$$
\begin{aligned}
& P(X=k)=P(X \leq k)-P(X \leq k-1) \\
& =\int_\lambda^{\infty}\left(f_{k+1}(y)-f_k(y)\right) d y \\
& =\int_\lambda^{\infty}(y-k) \frac{y^{k-1}}{k!} e^{-y} d y \\
& =\frac{1}{k!} \int_\lambda^{\infty} d\left(-y^k e^{-y}\right) \\
& =e^{-\lambda} \frac{\lambda^k}{k!}
\end{aligned}
$$
Таким образом у нас есть способ моделирования Пуассоновского распределения через экспоненциальное.
\end{proof}

\end{lemma}

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{3_3.eps} 
    \caption{Сравнение двух методов построения: на основе утверждения и поиск минимума по выборке}
\end{figure} 
\FloatBarrier




\subsection{Часть 3.4}
\begin{theorem}[Теорема Пуассона]
  Пусть есть последовательность серий испытаний Бернулли, где ${p_{n}}$ — вероятность «успеха», $\mu _{n}$ — количество «успехов».
$$
\lim_{n \to \infty} n p_n = \lambda ; \
\lambda > 0, \
\lim_{n \to \infty} P \bigl(\mu_n = m\bigr) = e^{-\lambda}, \cfrac {\lambda^m} {m!} .
$$
\end{theorem}
Таким образом, получаем ещё один способ моделирования случайной величины с экспоненциальным распределением.

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{3_4.eps} 
    \caption{Сравнение двух методов построения: библиотечного и по выборке}
\end{figure} 
\FloatBarrier




Критерий согласия Пирсона, или критерий согласия $\chi^2$ критерий для проверки гипотезы о принадлежности наблюдаемой выборки $x_1 , x_2 , ..., x_n$ объёмом n некоторому теоретическому закону распределения $F(x, \theta)$. Используем критерий для проверки простой гипотезы вида
$H_0 : F_n (x) = Pois(x, \lambda)$. \\
Исследуемое распределение принимает только целые неотрицательные значения.
Обозначим за $n_i$ количество элементов в выборке равных $i$.Теоритическая вероятность выпадения значения $i$ в распределение Пуассона:
$$
pi = \frac{\lambda_i}{i!} e^{-\lambda}.
$$
Пусть $k$ — максимальное значение в выборке. Построим статистику критерия $\chi^2$ Пирсона:
$$
  X_n^2 = n \sum_{i=1}^n \frac{\Big(\frac{n_i}{n} - p_i \Big)^2}{p_i}
$$  

Гипотеза о пуассоновском распределение построенной случайной величины будет
верна, если вычисленное значение статистики $X_n^2$ не превосходит критического значения  $\xi_{r,\alpha}^2$. Где $r = k - 1$, где $\alpha$ - заданный уровень значимости.


\begin{tabular}{|c|c|c|}
\hline
\multicolumn{3}{|c|}{Проверка датчика с помощью критерия Хи-квадрат Пирсона} \\
\hline
Число испытаний & Уровень значимости & Частота принятия гипотезы \\
\hline
100 & 5\% & 0.95 \\
1000 & 5\% & 0.911 \\
100 & 10\% & 0.86 \\
1000 & 10\% & 0.848 \\
\hline
\end{tabular}


\subsection{Часть 3.5}
\begin{statement}
  Пусть $\omega \sim \operatorname{Exp}(0.5)$, $\phi \sim \mathrm{U}[0,2 \pi]$. Тогда случайные величины $\xi=\sqrt{\omega} \cos \phi$, и $\eta=\sqrt{\omega} \sin \phi$ имеют стандартное нормальное распределение.
\end{statement}

\begin{proof}
 

$$
\begin{gathered}
\mathbb{P}(\xi<x, \eta<y)=\frac{1}{2 \pi} \int_{\infty}^x \int_{\infty}^y e^{-\frac{x_1^2+y_1^2}{2}} d x_1 d x_2=\{\text { переход в полярные координаты }\}= \\
=\frac{1}{2 \pi} \iint_{
\begin{array}{c}
r \cos (\phi)<x \\
r \sin (\phi)<y
\end{array}} r e^{-\frac{r^2}{2}} d r d \phi=\frac{1}{4 \pi} \iint_{\substack{\sqrt{\omega} \cos (\phi)<x \\
\sqrt{\omega} \sin (\phi)<y}} e^{-\frac{\omega}{2}} d \omega d \phi \\
\mathbb{P}(\xi<x, \eta<y)==\frac{1}{\sqrt{2 \pi}} \int_{\infty}^x e^{-\frac{x_1^2}{2}} d x_1 \frac{1}{\sqrt{2 \pi}} \int_{\infty}^y e^{-\frac{y_1^2}{2}} d y_1=\mathbb{P}(\xi<x) \mathbb{P}(\eta<y)
\end{gathered}
$$

\end{proof}


\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{3_5.eps} 
    \caption{Сравнение двух методов построения: библиотечного и по выборке($N=10^3$)}
\end{figure} 

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{3_5(1).eps} 
    \caption{Сравнение двух методов построения: библиотечного и по выборке($N=10^4$)}
\end{figure} 
\FloatBarrier


\begin{definition}
Пусть $Y_0,Y_1,\dots,Y_n$ --- независимые стандартные нормально распределенные случайные величины: $Y_i\sim\mathcal{N}(0,1),i=\overline{0,n}$. Тогда распределение случайной величины $t$:
$$
t = \dfrac{Y_0}{\sqrt{\frac{1}{n}\sum^n_1Y_i^2}}
$$
называется распределением Стьюдента с $n$ степенями свободы: $t\sim \mathrm{t}(n)$.
\end{definition}

\begin{statement}[Стьюдент]
Пусть $X^1_1,X^1_2,\dots,X^1_n, X^2_1,X^2_2,\dots,X^2_n$ - независимые случайные величины такие, что $X^j_i\sim\mathrm{N}(\mu,\sigma^2), i=\overline{1,n}, j=1,2$.
Положим $\overline{X}_1, \overline{X}_2$ - выборочное среднее, $S_1^2, S_2^2$ - несмещенная выборочная дисперсия. Тогда
$$
T=\dfrac{\overline{X}_1-\overline{X}_2}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}\sim \mathrm{t}(n-1),
$$
где $\mathrm{t}(n-1)$ --- распределение Стьюдента с $(n-1)$ степенями свободы.
\end{statement}

Проверим гипотезу о равенстве мат. ожидания конкретному значению одной нормально распределенной выборки при помощи t-критерия Стьюдента:
$$
H_0:~ \mathbb{E}\xi_1 = \mathbb{E}\xi_2.
$$

Согласно утверждению Стьюдента при истинности гипотезы $H_0$ статистика $T$ имеет распределение $\mathrm{t}(n-1)$. \
Отметим, что критерий Стьюдента имеет двустороннюю критическую область.

Будем принимать $H_0$, если $|T| < K_\alpha = F^{-1}_{t(df)}(1-\alpha/2)$, где $df$ - степени свободы. Были проведены эксперименты с двумя вариантами степеней свободы. Из классического $t$-критерия Стьюдента $df = n_1 + n_2 - 2$ (тут используем гипотезу о равенстве дисперсий) и второй вариант из критерия $Welch$, где нет такой гипотезы и количество степеней свободы $df =  \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}\right)^2}{\frac{\left(\frac{s_1^2}{n_1}\right)^2}{n_1-1} + \frac{\left(\frac{s_2^2}{n_2}\right)^2}{n_2-1}}$. 


\begin{tabular}{|c|c|c|}
\hline
\multicolumn{3}{|c|}{Проверка равенства математических ожиданий с помощью критерия Стьюдента ($Welch$)} \\
\hline
Число испытаний & Уровень значимости & Частота принятия гипотезы \\
\hline
100 & 5\% & 0.96(0.95) \\
1000 & 5\% & 0.962(0.955) \\
100 & 10\% & 0.9(0.88) \\
1000 & 10\% & 0.89(0.893) \\
\hline
\end{tabular}
В скобках указаны соответствующие значения для критерия $Welch$.

Проверка критерием Фишера гипотезы о равенстве дисперсий
гипотеза $ H_0:~ \sigma_X^2 = \sigma_Y^2$ о равенстве дисперсий двух независимых нормальных случайных величин $X$ и $Y$ с неизвестными дисперсиями $\sigma_X^2$ и $\sigma_Y^2$. Для исследования гипотезы берутся выборки для которых считаются выборочные оценки математических ожиданий и дисперсий, на основе которых
рассматривается статистика:
$$
  F = \frac{S_X^2}{S_Y^2}.
$$
Критерий Фишера тоже имеет двустороннюю критическую область.

\begin{tabular}{|c|c|c|}
\hline
\multicolumn{3}{|c|}{Проверка равенства дисперсий с помощью критерия Фишера} \\
\hline
Число испытаний & Уровень значимости & Частота принятия гипотезы \\
\hline
100 & 5\% & 0.96 \\
1000 & 5\% & 0.974 \\
100 & 10\% & 0.94 \\
1000 & 10\% & 0.956 \\
\hline
\end{tabular}


\section{Задание 4}


\begin{enumerate}
\item Построить датчик распределения Коши.
\item На основе датчика распределения Коши с помощью метода фон Неймана построить датчик стандартного нормального распределения. При помощи 
графика normal probability plot убедиться в корректности построенного датчика и обосновать наблюдаемую линейную зависимость.
\item Сравнить скорость моделирования стандартного нормального распределения в задании 3 и в задании 4.
\end{enumerate}

\subsection{Генератор распределения Коши}

\begin{definition}
Распределение Коши $C(x_0,\gamma)$ --- абсолютно непрерывное распределение с плотностью
$$
\dfrac{1}{\pi\gamma\left(1+(\frac{x-x_0}{\gamma})^2 \right)},
$$
где $x_0$ - параметр сдвига, $\gamma>0$ - параметр масштаба.
\end{definition}


Для генерации случайных чисел с распределением Коши используем метод обратного преобразования. \
Найдём функцию распределения $F(x)$, интегрируя плотность:
$$
    F(x) = \frac{1}{\pi}\arctan(\frac{x-x_0}{\gamma}) + \frac{1}{2}
$$

Найдём обратную функцию $F^{-1}(p)$:
$$
    F^{-1}(p) = x_0 + \gamma\tan(\pi(p-\frac{1}{2}))
$$
где $p \in (0,1)$

Для генерации случайной величины с распределением Коши: \
Генерируем $u \sim U(0,1)$ (равномерное распределение) 
$$
    X = x_0 + \gamma\tan(\pi(u-\frac{1}{2}))
$$


\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{4_1.eps} 
    \caption{Сравнение двух методов построения: библиотечного и по выборке($N=10^3$)}
\end{figure} 
\FloatBarrier


\subsection{}
Метод отбора с отвержением (Метод фон Неймана) является одним из основных методов генерации выборок из сложного распределения, когда прямая генерация выборок затруднительна. Этот метод основан на использовании более простого распределения для генерации кандидатных выборок и их последующего принятия или отклонения на основе определенного критерия.


1. Пусть у нас есть целевое распределение(нормальное в нашем случае) с плотностью вероятности $ f(x) $, из которого мы хотим получить выборки.

2. Выбираем простое распределение(распределение Коши) $g(x)$, из которого легко генерировать выборки. Это распределение должно охватывать область значений целевого распределения(оба распределения определены $\forall x \in \mathbb{R}$).

3. Находим константу $ M $, такую что:
   $$
   f(x) \leq M \cdot g(x),~ \forall x.
   $$
   Эта константа позволяет масштабировать предложенное распределение так, чтобы оно "покрывало" целевое.


Алгоритм можно описать следующими шагами:

   1. Генерируем случайное значение $ X $ из предложенного распределения $ g(x) $.
   2. Генерируем случайное значение $ U $ из равномерного распределения на интервале [0, 1].
   3. Принимаем значение $ X $, если выполняется условие:
   $$
   U \leq \frac{f(X)}{M \cdot g(X)}
   $$
   В противном случае отвергаем $ X $ и повторяем процесс.

Для обоих распределений максимум находится в нуле. 
$$
   f(0) = \frac{1}{\sqrt{2 \pi} },~ g(0) = \frac{1}{\pi} \\
   M = \sqrt{ \frac{\pi}{2}}
$$


\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{4_1(1).eps} 
    \caption{Сравнение двух методов построения: библиотечного и метода фон Неймана($N=10^3$)}
\end{figure} 
\FloatBarrier


\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{4_2.eps} 
    \caption{Q-Q plot}
\end{figure} 

Normal Probability Plot (Q-Q plot) задается следующим образом:

    По оси Y откладываются упорядоченные значения выборки, \\
    По оси X откладываются теоретические квантили стандартного нормального распределения, \\
    Если данные близки к нормальному распределению, точки будут располагаться вдоль прямой линии. \\

График линеен для $N(\mu, \sigma^2)$ потому что: 

Если $X \sim N(\mu, \sigma^2)$, то $X = \mu + \sigma Z$, где $Z \sim N(0,1)$. \\
Это линейное преобразование сохраняет линейность графика \\
    Наклон прямой равен $\sigma$ (стандартному отклонению), \\
    Сдвиг прямой равен $\mu$ (математическому ожиданию).

Для других распределений график нелинеен потому что: 

Квантили этих распределений не связаны линейным преобразованием с квантилями нормального распределения \\
Форма распределения отличается от нормального, что отражается в нелинейности графика.


График будет инвариантен для распределений из семейства местоположения-масштаба(Location-scale family): \\
В данном семействе выполняется замкнутость относительно линейных преобразований \\
В данное семейство входят: распределения Коши, t-распределения Стьюдента, и равномерное распределение.



\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{4_2(2).eps} 
    \caption{Q-Q plot}
\end{figure} 


\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{4_2(3).eps} 
    \caption{Q-Q plot}
\end{figure} 
\FloatBarrier

\subsection{Часть 4.3}
Продемонстрируем графики для сравнения двух методов: полярного и метода элиминации.
\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{4_3.eps} 
    \caption{Сравнение скорость моделирования выборки из стандартного нормального распределения полярным методом и методом фон Неймана.}
\end{figure} 
\FloatBarrier

\section{Задание 5}

\begin{enumerate}
\item Пусть $X_i\sim \mathcal{N}(\mu,\sigma^2)$. Убедиться эмпирически в справедливости теоремы о законе больших чисел (ЗБЧ) и центральной предельной теоремы (ЦПТ): исследовать поведение суммы $S_n = \sum\limits_{i=1}^n X_i$ и эмпирического распределения величины
$$
\sqrt{n}\dfrac{S_n}{n} - a.
$$
\item Считая $\mu$, $\sigma$ неизвестными, построить доверительные интервалы для среднего и дисперсии по имеющейся выборке.
\item Пусть $X_i \sim K(a,b)$ - имеет распределение Коши с параметрами сдвига $a$ и масштаба $b$. Изучить эмпирически как ведут себя суммы $\frac{S_n}{n}$, объяснить результат и найти закон распределения данных сумм.
\end{enumerate}
\subsection{}
\begin{theorem}[Закон болиших чисел]
Пусть $X_1, \ldots, X_n, \ldots$ - последовательность независимых одинаково распределённых случайных величин с конечным первым моментом, равным $a$. Обозначим, через
$$
S_n=\sum_{i=0}^n \frac{X_i}{n}
$$

Тогда для любого $\varepsilon>0$

$$
\lim _{n \rightarrow \infty} \mathbb{P}\left(\left|S_n-a\right|<\varepsilon\right)=1
$$

\end{theorem}

Рассмотрим последовательность случайных величин $X_1, \ldots, X_n, \ldots$, где $X_i \sim \mathrm{~N}\left(\mu, \sigma^2\right)$. Тогда математическое ожидание $X_i$ конечно и равно $\mu$ для всех $i=\overline{1, +\infty}$.
\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{5_1.eps} 
    \caption{}
\end{figure} 
\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{5_1(1).eps} 
    \caption{}
\end{figure} 

\FloatBarrier

\begin{definition}
 Пусть задано $0 < \alpha < 1$, тогда инвервал $(\theta_1, \theta_2)$ называется доверительным интервалом для параметра $\theta$ с уровнем
доверия $1 - \alpha$, если $\forall \theta \in \Theta$($\Theta$ - параметрическое семейство) выполняется:
$$
  \mathbb{P}(\theta_1 < \theta < \theta_2) \geqslant 1 - \alpha.
$$
\end{definition}
\begin{corollary}
 Если неравенство выполнено при $n \to +\infty$
 для последовательности интервалов $(\theta_{1_n}, \theta_{2_n})$, то интервал называется асимптотическим.
\end{corollary}

\subsection{Построение доверительных интервалов}
Построение доверительных интервалов для $\mu$ и $\sigma^2$ Рассмотрим $X_1, \ldots, X_n$ последовательность н.о.р.с.в., где $X_k \sim N\left(\mu, \sigma^2\right)$. Оценим параметры $\mu$ и $\sigma^2$ по выборке.

Выпишем доверительный интервал для математического ожидания $\mu$. Обозначим за квантили порядка $\alpha$ распределения Стьюдента с числом степеней свободы $n-1$, как $s t_{\alpha, n-1}$. Тогда рассмотрим случайную величину $Y=(\bar{X}-\mu) \cdot \frac{\sqrt{n}}{S} \sim S t(n-1)$ и выпишем для неё определения квантилей в виде вероятности попадания в заданный интервал:

$$
\mathbb{P}\left(s t_{\frac{1-\alpha}{2}, n-1} \leq Y \leq s t_{\frac{1+\alpha}{2}, n-1}\right)=\alpha
$$


В силу симметричности распределения можно записать, что $s t_{1-\alpha, n-1}=-s t_{\alpha, n-1}$, поэтоmy

$$
\begin{gathered}
\mathbb{P}\left(-s t_{\frac{1+\alpha}{2}, n-1} \leq Y \leq s t_{\frac{1+\alpha}{2}, n-1}\right)=\alpha \\
\mathbb{P}\left(-s t_{\frac{1+\alpha}{2}, n-1} \leq(\bar{X}-\mu) \cdot \frac{\sqrt{n}}{S} \leq s t_{\frac{1+\alpha}{2}, n-1}\right)=\alpha \\
\mathbb{P}\left(-\bar{X}-\frac{S}{\sqrt{n}} \cdot s t_{\frac{1+\alpha}{2}, n-1} \leq-\mu \leq-\bar{X}+\frac{S}{\sqrt{n}} \cdot s t_{\frac{1+\alpha}{2}, n-1}\right)=\alpha \\
\mathbb{P}\left(\bar{X}-\frac{S}{\sqrt{n}} \cdot s t_{\frac{1+\alpha}{2}, n-1} \leq \mu \leq \bar{X}+\frac{S}{\sqrt{n}} \cdot s t_{\frac{1+\alpha}{2}, n-1}\right)=\alpha
\end{gathered}
$$


Выпишем доверительный интервал для дисперсии $\sigma^2$. Обозначим за квантили порядка $\alpha$ распределения хи квадрат с числом степеней свободы $n-1$, как $\chi_{\alpha, n-1}^2$. Тогда рассмотрим случайную величину $V=(n-1) \cdot \frac{S^2}{\sigma^2} \sim \chi^2(n-1)$ и выпишем для неё определения квантилей в виде вероятности попадания в заданный интервал:

$$
\begin{gathered}
\mathbb{P}\left(\chi_{\frac{1-\alpha}{2}, n-1}^2 \leq V \leq \chi_{\frac{1+\alpha}{2}, n-1}^2\right)=\alpha \\
\mathbb{P}\left(\chi_{\frac{1-\alpha}{2}, n-1}^2 \leq(n-1) \cdot \frac{S^2}{\sigma^2} \leq \chi_{\frac{1+\alpha}{2}, n-1}^2\right)=\alpha \\
\mathbb{P}\left(\frac{(n-1) \cdot S^2}{\chi_{\frac{1+\alpha}{2}, n-1}^2} \leq \sigma^2 \leq \frac{(n-1) \cdot S^2}{\chi_{\frac{1-\alpha}{2}, n-1}^2}\right)=\alpha
\end{gathered}
$$

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{5_2.eps} 
    \caption{}
\end{figure} 

\FloatBarrier

\subsection{}
Пусть $X_1, X_2, \ldots, X_n$ — независимые случайные величины, имеющие распределение Коши с параметрами $a$ и $b$, то есть $X_i \sim K(a, b)$ для $i = 1, 2, \ldots, n$. 

Сумма этих случайных величин определяется как:

$$
S_n = X_1 + X_2 + \ldots + X_n.
$$


Свойство устойчивости распределения Коши заключается в том, что сумма независимых случайных величин, имеющих это распределение, также будет иметь распределение Коши. Формально это можно записать следующим образом:
$$
S_n \sim K(na, nb).
$$


Это означает, что если $X_i$ имеют распределение Коши с параметрами $a$ и $b$, то нормированная сумма $S_n / n \sim K(a, b)$.


Характеристическая функция распределения Коши имеет вид:

$$
\phi(t) = \exp \{ x_0 i t - \gamma |t|\}.
$$


Это свойство позволяет показать, что сумма независимых случайных величин с этим распределением также будет иметь характеристическую функцию того же вида.

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{5_3.eps} 
    \caption{}
\end{figure} 

\FloatBarrier



\section{Задание 6}
Необходимо вычислить следующий интеграл:
$$
I = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}
\dfrac{e^{-(x_1^2+x_2^2+\dots+x_{10}^2 + \frac{1}{2^7 x_1^2\cdots x_{10}^2})}}{x_1^2\cdot\ldots\cdot x_{10}^2}
dx_1dx_2\dots dx_{10}
$$

1. Методом Монте-Карло, \\
2. Методом квадратур, сводя задачу к вычислению собственного интеграла Римана.

Также необходимо оценить точность вычислений для каждого из двух случаев.

\subsection{}
Перепишем интеграл немного в другом виде.\\
Заметим, что в подинтегральной функции можно выделить
$$
    \rho(x_1,x_2,\dots,x_{10}) = \frac{1}{\sqrt{\pi}^{10}}\exp\left\{ -\sum_{i=1}^{10}x_i^2 \right\},
$$
где $\rho(x_1,x_2,\dots,x_{10})$ - плотность нормального распределения $\eta \sim N(0, \sigma^2 = 1/2)$. \\
Тогда оставшуюся часть можно записать в ф-ию 
$$
    f(x_1,x_2,\dots,x_{10}) = \exp\left\{- \frac{1}{2^7 \prod_{i=1}^{10}x_i^2} \right\} ({\prod_{i=1}^{10}x_i^2})^{-1} 
$$
Теперь перепишем получившийся интеграл
$$
\begin{aligned}
I= \pi^5\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}f(x_1,x_2,\dots,x_{10}) \rho(x_1,x_2,\dots,x_{10}),
dx_1dx_2\dots dx_{10},
\end{aligned}
$$
который равняется $\mathbb{E}[f(\eta)]$. \\
Таким образом получили первый способ моделирования.

Для $N = 10 ^ 7,~ I_N = 125.208$.

\subsection{}
Теперь решим задачу методом прямоугольников, для этого нужно сделать замену(интеграл не собственный). \\
Во-первых, обозначит подынтегральную функцию $F(x_1,x_2,\ldots,x_{10})$, и заметим, что $F(x_1,x_2,\ldots,-x_i ,\ldots,x_{10}) = F(x_1,x_2,\ldots,x_i ,\ldots,x_{10}),~ \forall i=1, \ldots, 10$. \
Таким образом можно считать интеграл только по неотрицательным значениям и потом домножить на 2 каждый интеграл.  \\
Рассмотрим замену $x_i = \tg(\frac{\pi}{2}\phi_i),~i=1,\ldots,10$. Такая замена может напоминать универсальную тригонометрическую подстановку ($dx_i = \frac{\pi}{2} \frac{1}{\cos^{2}({\frac{\pi}{2}} \phi_i)}d\phi_i $). \\
Теперь область интегрирования будет $[0;1]^{10}$, значения тангенса будут из первой четверти. 


Интеграл равен
$$
\begin{aligned}
I &= \pi^{10}\int_{0}^1\int_{0}^{1}\cdots\int_{0}^1 g(\phi_1,\phi_2,\ldots,\phi_{10})d\phi_1d\phi_2\dots d\phi_{10}, \\
g(\phi_1,\phi_2,\ldots,\phi_{10}) &= \frac{1}{\prod^{10}_{i=1}\cos^2\left(\frac{\pi}{2}\phi_i\right) \tg^2\left(\frac{\pi}{2}\phi_i\right)}
\exp\left\{ -\left( \sum_{i=1}^{10}\tg^2\left(\frac{\pi}{2}\phi_i\right) + \frac{1}{2^7}\prod^{10}_{i=1} \tg^{-2}\left(\frac{\pi}{2}\phi_i\right) \right) \right\}.
\end{aligned}
$$
Учтём, что 
$$
    \cos(x) \tg(x) = \sin(x)
$$

Воспользуемся методом прямоугольников: разобъём отрезки $[0,1]$ на $N$ равных частей:
$$
0=x_k^0<x_k^2<\ldots<x_k^{2 N}=1,~ k=1, \ldots, 10.
$$

На каждой части разбиения каждого из десяти отрезков $[0,1]$ возьмём середину:
$$
x_k^1, x_k^3, \ldots, x_k^{2 N-1},~ k=1, \ldots, 10.
$$

Получается записать интеграл можно в виде
$$
    I \approx \left( \dfrac{\pi}{N} \right)^{10} \sum_{i=1}^{N}\dots\sum_{i_{10}=1}^{N} g(x_1^{2i_1 - 1}, x_2^{2i_2 - 1}, \dots, x_{10}^{2i_{10} - 1})
$$

Теперь заметим, что нет разницы в каком порядке интегрировать. Так же, не влияет на результат перестановки вида 
$$
g(\phi_1,\phi_2,\ldots, \phi_i,\ldots, \phi_j, \ldots,\phi_{10}) = g(\phi_1,\phi_2,\ldots, \phi_j,\ldots, \phi_i, \ldots,\phi_{10}),
~ \forall i, j:~ i,j = 1, \ldots, 10,~ i \ne j.
$$
Если представить значение ф-ии в узлах как тензор, то он получится симметричный.

Данное замечание позволяет нам группировать индексы 
Мы можем представить сумму в виде $1\leq i_0\leq i_1\leq\dots \leq i_{10}\leq N$, а значения в узлах домножать на количество сочетаний. \\
Пусть у нас $l$ - различных значений, тогда будет сочетание вида
$$
 C^{m}_{m_1, \ldots, m_l} = \frac{10!}{m_1!m_2!\cdots m_{l}!},
$$
где $m_k!$ - количество индексов в наборе $(i_1,i_2,\dots,i_{10})$, равных $k$.

$$
    I \approx \left( \dfrac{\pi}{N} \right)^{10} \sum_{1\leq i_0\leq i_1\leq\dots \leq i_{10}\leq N} C^{m}_{m_1, \ldots, m_l} ~ g(x_1^{2i_1 - 1}, x_2^{2i_2 - 1}, \dots, x_{10}^{2i_{10} - 1})
$$


\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{6_2.eps} 
    \caption{Аппроксимация с помощью метода Монте-Карло}
\end{figure} 

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{6_2(2).eps} 
    \caption{Аппроксимация с помощью метода прямоугольников}
\end{figure} 

\FloatBarrier

\section{Задание 7}
Необходимо методом случайного поиска реализовать поиск минимальное значение функции. 
$$
    f(x_1, x_2) = x_1 ^ 3 \sin{ \Big( \frac{1}{x_1} \Big)} + 10x_1 x_2^4 \cos{\Big( \frac{1}{x_2} \Big)},~ x_1, x_2 \ne 0,
$$
При $x_1 = 0$ или $x_2 = 0$ функция доопределяется по непрерывности.

Рассмотрим множество, на котором необходимо оптимизировать функцию
$$
    A = \{ x_1, x_2 \in \mathbb{R}^2: x_1^2 + x_2 ^ 2 \leqslant 1 \}.
$$
\subsection{}
$
    \forall (x_1, x_2) \in \mathbb{R}^2 \backslash (\{ x_1 = 0, x_2 \in \mathbb{R} \} \cup \{x_1 \in \mathbb{R}, x_2 = 0 \}),~ f(x_1, x_2)
$   - функция непрерывна. \\
Докажем непрерывность на всей плоскости.
$$
    \lim_{x_{1} \to 0} x_1 ^ 3 \sin{ \Big( \frac{1}{x_1} \Big)} + 10x_1 x_2^4 \cos{\Big( \frac{1}{x_2} \Big)} = 
$$
$$
    \{ \text{Получаем бесконечно малую ($x_1^3$) на ограниченную последовательность $ \sin{ \Big( \frac{1}{x_1} \Big)}$}  \} = 
    0.
$$
Получили непрерывность по $x_1$, аналогично для $x_2$ и точки $(0,0)$ получаем непрерывность ф-ии на всём множестве. \\
Соответственно, непрерывная функция на ограниченном множестве достигает своих максимума и минимума. 

Заметим, что $f(x_1, x_2) = f(x_1, -x_2)$, соответственно экстремумов будет минимум 2.\\
Для поиска значения будем генерировать сразу $n$ точек на плоскости и будем искать минимальное значение, вычисляя $f(x_1^{i}, x_2^{i})$. \\
Для оценки точности будем использовать параметр уверенности попадания(вероятность попадания $p$) в $\varepsilon$-окрестность минимума. 
Оценим вероятность того, что определённая точка попадёт в нужную окрестность одного из минимумов.
$$
    P((x_1, x_2) \in B_{\varepsilon}(x_1^{*}, x_2^{*})) = \frac{\pi \cdot \varepsilon^2}{\pi \cdot 1} = \varepsilon^2.
$$
Рассмотрим событие $A$ - $\big(x^1 \in B_{\varepsilon}(x_1^{*}, x_2^{*}) \mid \ldots \mid x^n \in B_{\varepsilon}(x_1^{*}, x_2^{*})\big)$ хотя бы одна из точек попадёт. \
Воспользуемся свойством $P(C) + P(\overline{C}) = 1$, где $\overline{C}$ - дополнение в произвольному событию $C$. \
Введём событие $B_i$ - $\big(x^i \in B_{\varepsilon}(x_1^{*}, x_2^{*}) \big)$ -  $i$-ая точка попала в окр-ть.
$$
P(\overline{A}) = \prod\limits_{i=1}\limits^{n} P(\overline{B_i}) = (P(\overline{B_i}))^n.
$$
Таким образом получаем
$$
    p = P(A) = 1 - (1- \varepsilon^2)^n. \\
    n =  \Big\lceil \frac{\ln(1-p)}{1 - \varepsilon^2} \Big\rceil
$$
Оценим погрешность вычисления. 
$$
    \frac{\partial f}{\partial x_1}(x_1, x_2) = 3x_1 ^ 2 \sin{ \Big( \frac{1}{x_1} \Big)} - x_1 \cos{ \Big( \frac{1}{x_1} \Big)} + 10 x_2^4 \cos{\Big( \frac{1}{x_2} \Big)},
$$
$$
    \frac{\partial f}{\partial x_2}(x_1, x_2) = 40x_1 x_2^3 \cos{\Big( \frac{1}{x_2} \Big)} + 10x_1 x_2^2 \sin{\Big( \frac{1}{x_2} \Big)} \\
$$

$$
 \mid f(x_1, x_2) - f(x_1^{*}, x_2^{*}) \mid =  \mid f(x_1, x_2)-f(x_1,x_2^{*})+  f(x_1, x_2^{*})-f(x_1^{*}, x_2^{*}) \mid \leqslant 
$$
$$
 \leqslant \max \limits_{x \in A}\left|\frac{\partial f}{\partial x_2}(x_1, x_2) \left\|x_2^*- x_2\left|+\max \limits_{x \in A}\right| \frac{\partial f}{\partial x_1}(x_1, x_2) \right\| x_1^*- x_1\right|
$$


Оценим частные производные функции $f$ :
$$
\begin{aligned}
&\left| \frac{\partial f}{\partial x_1}(x_1, x_2) \right|=\left|3 x_1^2 \sin \left(\frac{1}{x_1}\right)-x_1 \cos \left(\frac{1}{x_1}\right)+10 x_2^4 \cos \left(\frac{1}{x_2}\right)\right| \leqslant\left\{\left(x_1, x_2\right) \in A\right\} \leqslant \\
& \leqslant\left|3 \sin \left(\frac{1}{x_1}\right)-\cos \left(\frac{1}{x_1}\right)+10 \cos \left(\frac{1}{x_2}\right)\right| \leqslant \\
& \left\{ 3^2 + 1^2 = 10,  \text{ делим и домножаем на корень из этого числа} \right\} \leqslant 10+\sqrt{10},
\end{aligned}
$$


$$
\begin{aligned}
& \left| \frac{\partial f}{\partial x_2}(x_1, x_2) \right|=10\left|x_1\left(4 x_2^3 \cos \left(\frac{1}{x_2}\right)+x_2^2 \sin \left(\frac{1}{x_2}\right)\right)\right|  \leqslant\left\{\left(x_1, x_2\right) \in A\right\} \leqslant \\
 & \leqslant 10\left|4 \cos \left(\frac{1}{x_2}\right)+\sin \left(\frac{1}{x_2}\right)\right| \leqslant \left\{ 4^2 + 1^2 = 17,\text{ делим и домножаем на корень из этого числа} \right\} \leqslant 10 \sqrt{17}
 \end{aligned}
$$

Итак, осталось воспользоваться ещё раз тем, что это координаты из $\varepsilon$-окрестности
$$
\mid f(x_1, x_2) - f(x_1^{*}, x_2^{*}) \mid \leqslant (10+\sqrt{10} + \sqrt{17}) \varepsilon
$$

Теперь учтём полученную ранее оценку и выпишем оценку как зависимость от $n$ и $p$:
$$
    \mid f(x_1, x_2) - f(x_1^{*}, x_2^{*}) \mid \leqslant (10+\sqrt{10} + \sqrt{17}) \sqrt{1 - (1 - p)^{1/n}}
$$

мн-во $A$ это шар единичного радиуса $B_1(0)$. Поэтому воспользуется переходом в полярные координаты 
$$
    \begin{cases}
        x_1 = r \cos{\varphi}, \\
        x_2 = r \sin{\varphi}.
    \end{cases}
$$
Соответственно, перепишем множество и функцию через полярные координаты
$$
    A = \{ r \in [0;1],~ \varphi \in [0; 2\pi) \}.    
$$

$$
f(r, \varphi) = r^3 \cos^3{\varphi} \sin{\Big( \frac{1}{r \cos{\varphi}} \Big)} + 10r^5 \cos{\varphi} \sin^4{\varphi} \cos{\Big( \frac{1}{r \sin{\varphi}} \Big)}.
$$


$$
|J| = \begin{vmatrix}
    \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \varphi} \\
    \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \varphi}
\end{vmatrix} =
\begin{vmatrix}
    \cos \varphi & -r \sin \varphi \\
    \sin \varphi & r \cos \varphi
\end{vmatrix} = r \cos^2 \varphi + r \sin^2 \varphi = r
$$

Таким образом, якобиан равен \( r \). Если у нас есть плотность вероятности $f(x_1, x_2) $ в декартовых координатах, то плотность вероятности $ g(r, \varphi) $ в полярных координатах будет:

$$
g(r, \varphi) = f(r \cos \varphi, r \sin \varphi) \cdot |J| = f(r \cos \varphi, r \sin \varphi) \cdot r.
$$
Будем моделировать $r^2 = u \sim U[0;1],~ \varphi \sim U[0;2\pi]$. \
Выпишем совместную плотность и вероятность попадания вектора во множество:
$$
\begin{aligned}
    & p_A(x_1, x_2) = \frac{1}{\pi} I \{ x_1^2 + x_2^2 \leqslant 1 \}, \\
    & P\left(\left(x_1, x_2\right) \in A\right)=\iint\limits_{x_1^2+x_2^2 \leqslant 1} \frac{1}{\pi} d x_1 d x_2=\frac{1}{\pi} \cdot \int_0^1 r d r \int_0^{2 \pi} d \varphi=\int_0^1 d r^2 \int_0^{2 \pi} \frac{1}{2 \pi} d \varphi=\int_0^1 d u \int_0^{2 \pi} \frac{1}{2 \pi} d \varphi .
\end{aligned}
$$

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{7_1.eps} 
    \caption{Значение функции на множестве $A$}
\end{figure} 

\FloatBarrier

\subsection{Метод имитации отжига(simulated annealing)}
Метод имитации отжига(simulated annealing) для функции Розенборка
$$
    g(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2.
$$
Минимальное значение функции достигается в точке $x^{*} = (1, 1),~ g(x^{*}) = 0$

В самом начале задаём входные параметры $m$, $\sigma$, $t_0$ и начальное значение $(x_0, y_0)$.

Кандидата на следующую точку вычисляем по правилу:
$$
\begin{aligned}
    & x_{i+1} \sim N ( x_i, \sigma^2 T_i). \\
    & y_{i+1} \sim N ( y_i, \sigma^2 T_i).
\end{aligned}
$$
Для определения будет ли осуществляться переход вычисляем приращение функции $\Delta g$ при переходе в следующую точку. \
Если $\Delta g \leqslant 0$, то мы переходим в новую точку. \
Иначе переход осуществляется с вероятностью $p_k = \frac{1}{2} \exp(- \frac{\Delta g}{t_k})$. \
Температуру будем понижать по закону
$$
    T_{k+1} = k T_k.
$$
Медленно понижаем температуру, если приращения функции большие.

Для демонстрации приведён ниже 2 графика

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{7_1(point).eps} 
    \caption{Распределение ошибки дистанции для точки решения}
\end{figure} 

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{7_1(value).eps} 
    \caption{Распределение ошибки значения функции для решения}
\end{figure} 
\FloatBarrier

Заметим, что алгоритм выдаёт корректно, однако сущесутвуют точки, на которых происходят фатальные ошибки и получается большое отклонение от решения. 

\begin{tabular}{|c|c|c|}
\hline
\multicolumn{3}{|c|}{Проверка работы алгоритма имитации отжига} \\
\hline
Число испытаний & Среднее значение & Медианное значение \\
\hline
100 & 0.0791918136723309 & 1.7735270887528834e-08 \\
1000 & 0.0673130779161849 & 2.5976391152746705e-08 \\
10000 & 0.06691712375454373 & 2.5927705760100567e-08 \\
\hline
\end{tabular}


\section{Задание 8}
1. Применить метод Монте-Карло к решению первой краевой задачи для двумерного уравнения Лапласа в единичном круге
$$
    \begin{cases}
    \Delta u=0,~ (x, y) \in D \\
    \left.u\right|_{\delta D}=f(x, y) \\
    u \in C^2(D), f \in C(\delta D) \\
    D=\left\{(x, y) \in \mathbb{R}^2: x^2+y^2 \leqslant 1\right\}
    \end{cases}
$$
2. Для функции $f(x, y)=x^2-y^2$ найти аналитическое решение и сравнить с полученным по методу Монте-Карло.

\subsection{}
Воспользуемся переходом в полярные координаты
$$
    \begin{cases}
        x = r \cos{\varphi}, \\
        y = r \sin{\varphi}.
    \end{cases}
$$

Таким образом перепишем задачу
$$
    \begin{cases}
    \Delta u=0,~ (x, y) \in D \\
    u(1, \varphi)= g(\varphi) \\
    u \in C^2(D), f \in C([0; 2\pi]) \\
    D= \{ r \in [0;1],~ \varphi \in [0;2\pi] \}
    \end{cases}
$$

Решим для данной функции $f(x,y)$. Перепишем её в полярных координатах
$$
    g(\varphi) = \cos(2\varphi).
$$
$$
    \begin{aligned}
    & \alpha_0 = \frac{1}{2 \pi} \int_0^{2 \pi} \cos 2 \varphi d\varphi=\frac{1}{2 \pi} \cdot-\left.\sin 2\right|_0 ^{2\pi}=0 \\
    & \alpha_2 = \frac{1}{\pi} \int_0^{2\pi} \cos ^2(2 \varphi) d\varphi=\frac{1}{2\pi} \int_0^{2 \pi}(1+\cos (4\varphi) ) d \varphi = 1 \\
    \end{aligned}
$$
$$
    \forall n : n \ne 2 \implies \alpha_n = 0, \\
    \forall n \implies \beta_n = 0. \\
$$
Получаем решение в виде
$$
    u(r, \varphi)=r^2 \cdot \cos 2 \varphi = r^2 (\cos^2(\varphi) - \sin^2(\varphi)), \\
    u(x, y) = x^2 - y^2.
$$

Опишем, как здесь применяется метод Монте-Карло. \\
Рассматриваем разностную схему на двумерной сетке $x_{ij} = (-1 + ih, -1 + jh)$, шаг $h$

Рассматриваем случайное блуждание
$$
  v_{i,j} = \frac{1}{4}( v_{i-1,j} + v_{i+1,j} + v_{i,j-1} + v_{i,j+1}),
$$
где температуру(значение функции) из краев переносится с центр. 

Выполним моделирование 

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{8_1.eps} 
    \caption{Тепловые карты для решений и распределения ошибок на плоскости }
\end{figure} 

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{8_2.eps} 
    \caption{Тепловые карты для решений и распределения ошибок}
\end{figure} 
\FloatBarrier


\section{Задание 9}
Рассмотреть два вида гауссовских процессов: \\
\begin{itemize}
 \item Винеровский процесс $W (t), t \in [0; 1], W(0) = 0$.  
 \item Процесс Орнштейна–Уленбека $X(t), t \in [0; 1], X(0) = X_0$ , т.е. стационарный марковский гауссовский процесс.
\end{itemize}
Начальные значения $X_0$ следует выбирать случайным образом так, чтобы полученный процесс был стационарным.\\
Для данных процессов: \\
\begin{enumerate}
 \item Найти ковариационную функцию и переходные вероятности. 
 \item Промоделировать независимые траектории процесса с данными переходными
вероятностями методом добавления разбиения отрезка. 
  \item Построить график траектории, не соединяя точки ломаной, с целью получения
визуально непрерывной линии. 
\end{enumerate}

\subsection{Винеровский процесс}

Винеровский процесс — процесс такой, что
\begin{enumerate}
 \item $W(0) \stackrel{\text{ п.в.} }{=} 0$,
 \item $W(t+h) - W(t) \sim N(0, \sigma^2h)$,
 \item $W(t)$ - процесс с независимыми приращениями.
\end{enumerate}

Теперь, пользуясь определением найдём ковариационную функцию $\mathbb{K}(t_1, t_2)$ при условии $t_1 < t_2$:
$$
    \begin{aligned}
    &= \mathbb{K}(t_1, t_2) = \mathbb{E}[W(t_1)W(t_2)] = \\
    &= \Big\{ W(t_1)W(t_2) = W(t_1) \big(W(t_2) - W(t_1) + W(t_1) \big) = 
     (W(t_1))^2 + W(t_1) \big(W(t_2) - W(t_1)) \Big\} =\\
    &= \mathbb{E}[W^2(t_1)] + \mathbb{E}[(W(t_1) - W(0))(W(t_2) - W(t_1))] = 
     \{ \text{процесс~ с~ независимыми~ приращениями} \} = \\
    &= \sigma^2 t_1 + \mathbb{E}[W(t_1) - W(0)]\mathbb{E}[W(t_2) - W(t_1)] = 
    \{ \text{ воспользовались~ вторым~ пунктом~ определения.} \} = \\
    &= \sigma^2 t_1. 
    \end{aligned}
$$

Таким образом, ковариационная функция винеровского процесса имеет вид:
$$
    \mathbb{K}(t_1, t_2) = \min(t_1, t_2)\sigma^2.
$$
По определению известно, что $W(0) = 0$ и $[W(t) \mid W(0)] \sim N(0, \sigma^2t)$. \
Промоделируем переходные вероятности \
Пусть $\exists t_1, t_3 \in [0;1] : t_1 \ne t_3$, зная значения $W(t_1), W(t_3)$ в эти моменты времени найдём значение в точке $t_2 = \frac{t_1 + t_3}{2}$. 
$$
    Z = (W(t_2) \mid W(t_1)=w_1, W(t_3) = w_3).
$$
Воспользуемся тем фактов, что Винеровский процесс является гауссовским
$$
    \rho_Z(w_2) = \frac{\rho_{t_1,t_2,t_3}(w_1, w_2, w_3)}{\rho_{t_1, t_3}(w_1, w_3)},
$$
где $\rho_{t_1, t_2, t_3}(w_1, w_2, w_3)$ - совместная плотность значений винеровского процесса во всех трёх моментах времени, а $\rho_{t_1, t_3}(w_1, w_3)$ - значения, в двух известных моментах времени. \
Плотность многомерного нормального распределения с нулевым средним и ковариационной матрицей $\Sigma$ имеет вид
$$
    \rho(w_1, w_2, \dots, w_n) = \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} \exp \Big\{- \frac{1}{2} x\Sigma x^T\Big\}.
$$
Теперь, запишем ковариационные матрицы для обоих векторов

$$
\Sigma_{t_1,t_3} = \sigma^2
\begin{pmatrix}
t_1 & t_1 \\
t_1 & t_3
\end{pmatrix}
$$

$$
\Sigma_{t_1,t_2, t_3} = \sigma^2
\begin{pmatrix}
t_1 & t_1 & t_1 \\
t_1 & t_2 & t_2 \\
t_1 & t_2 & t_3
\end{pmatrix}
$$

С помощью символьных вычислений получим плотность $Z$. \
Для упрощения сразу воспользуюсь тем, что $t_2 = t_1 + h, t_3 = t_1 + 2h$.  

Обозначим $ {d_2'} = \frac{\sigma^2 h}{2}$. \\
Тогда перепишем показатель экспоненты в виде
$$
    - \frac{(w_2 - \frac{w_1 + w_3}{2}) ^ 2}{2d_2'}.
$$
Соответственно, получаем, что
$$
    Z \sim N \Big(\frac{w_1 + w_3}{2}, \sqrt{d_2'} \Big).
$$

Промоделирируем график

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{9_1.eps} 
    \caption{}
\end{figure} 
\FloatBarrier

\subsection{Процесс Орнштейна-Уленбека}

Воспользуемся стационарностью процесса Орнштейна-Уленбека. Введём обозначения
$$
    \mathbb{E}X_t = a = const,~ Var(X_t) = \sigma^2 = const,~ \mathbb{K}(t, s) = k(|t-s|).
$$
Исходя из стационарности ковариационная функция зависит только от разности аргументов, а следовательно и корреляционная $R(t, s) = \frac{\mathbb{K}(t,s)}{\sqrt{Var(X_s)} \sqrt{Var(X_t)}} = r(|t-s|)$. \
Воспользуемся свойством такого процесса, что 
$$
    R(X(t_1), X(t_3)) = R(X(t_1), X(t_2)) R(X(t_2), X(t_3)), ~ t_1 < t_2 < t_3.
$$
Запишем корреляционную функцию в виде
$$
\begin{aligned}
    & r(t_2) = R(X(t_2), X_0) = \{ \text{ теперь~ воспользуемся~ свойством } \} = \\
    & R(X(t_2), X(t_1)) R(X(t_1), X_0) = r(t_2 - t_1) r(t_1).
\end{aligned}
$$
Получаем уравнение (после переобозначения), которое будет являться функциональным уравнением Коши
$$
    r(t+ \tau) = r(t) r(\tau).
$$
Для которого в классе непрерывных функций существует нетривиальное решение в виде $r(t) = \exp^{ - \theta x},~ \theta > 0$. \\
Таким образом, ковариационная функция принимает вид
$$
    R(t, s) = \sigma^2 \exp^{- \theta |t-s|}.
$$

Промоделируем переходные вероятности для начального случая. \\
$X_0 \sim N(0, \sigma^2),~ $ Ковариационная матрица имеет вид
$$
\Sigma_{0,1} = \sigma^2
\begin{pmatrix}
1 & e^{-\theta} \\
e^{-\theta} & 1
\end{pmatrix}
$$


Осталось домножить и числитель, и знаменатель на $e^{-2\theta}$
$$
    \Big( X_1 \mid X_0 \Big) \sim N(w_0 e^{-\theta}, \sigma^2 (1 -e^{-2 \theta}))
$$ 

Промоделируем переходные вероятности для промежуточных моментов \
Пусть $\exists t_1, t_3 \in [0;1] : t_1 \ne t_3$, зная значения $W(t_1), W(t_3)$ в эти моменты времени найдём значение в точке $t_2 = \frac{t_1 + t_3}{2}$. 
$$
    Z = (W(t_2) \mid W(t_1)=w_1, W(t_3) = w_3).
$$
Воспользуемся тем фактов, что Винеровский процесс является гауссовским
$$
    \rho_Z(w_2) = \frac{\rho_{t_1,t_2,t_3}(w_1, w_2, w_3)}{\rho_{t_1, t_3}(w_1, w_3)},
$$
где $\rho_{t_1, t_2, t_3}(w_1, w_2, w_3)$ - совместная плотность значений винеровского процесса во всех трёх моментах времени, а $\rho_{t_1, t_3}(w_1, w_3)$ - значения, в двух известных моментах времени. \
Плотность многомерного нормального распределения с нулевым средним и ковариационной матрицей $\Sigma$ имеет вид
$$
    \rho(w_1, w_2, \dots, w_n) = \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} \exp \Big\{- \frac{1}{2} x\Sigma x^T\Big\}.
$$
Теперь, запишем ковариационные матрицы для обоих векторов.

Для упрощения сразу воспользуюсь тем, что $t_2 = t_1 + h, t_3 = t_1 + 2h$.  

$$
\Sigma_{t_1,t_2, t_3} = \sigma^2
\begin{pmatrix}
1 & e^{-\theta h} & e^{-2\theta h} \\
e^{-\theta h} & 1 & e^{-\theta h} \\
e^{-2\theta h} & e^{-\theta h} & 1
\end{pmatrix}
$$

С помощью символьных вычислений получим плотность $Z$. 

Параметры распределения 
$$
    Z \sim N \Big(\frac{w_1 + w_3}{e^{\theta h} + e^{-\theta h}}, \sigma^2 \frac{e^{\theta h} - e^{-\theta h}}{e^{\theta h} + e^{-\theta h}} \Big)
$$

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{9_2.eps} 
    \caption{Процесс Орнштейна-Уленбека}
\end{figure} 
\FloatBarrier


\section{Задание 10}

В задании следует рассмотреть дискретный одномерный фильтр Калмана для динамической системы вида:
$$
\begin{aligned}
x_{n+1} & =a \cdot x_n+\nu_n, \quad \nu_n \sim \mathcal{N}(0, q), \quad x_1 \sim \mathcal{N}\left(0, \sigma^2\right) \\
y_n & =x_n+\varepsilon_n, \quad \varepsilon_n \sim \mathcal{N}(0, r)
\end{aligned}
$$

Значение дисперсии шума $r$ считается известным еще на этапе генерации этого шума.
Для решения задачи рассмотрим равномерную сетку
$$
    h = \frac{1}{N},~ \{t_n = nh,~ n=\overline{0,1} \}, \\
    x_n = X(t_n),
$$
где $x_n$ - значения процесса Орнштейна-Уленбека в момент времени $t_n$.

Решим систему уравнений относительно неизвестных коэффициентов $a$ и $q$ через известные $\sigma$ и $\theta$:
$$
\left\{
    \begin{array}{l}
    \sigma^2=R\left(t_n, t_n\right)=Var\left(x_n\right) \\
    \sigma^2 e^{-\theta h}=R\left(t_n, t_{n+1}\right)=\operatorname{Cov}\left(x_n, x_{n+1}\right)=a \cdot Var\left(x_n\right) \\
    \sigma^2=R\left(t_{n+1}, t_{n+1}\right)=Var\left(x_{n+1}\right)=a^2 \cdot Var\left(x_n\right)+q
    \end{array}\right.
$$
После решения данной системы, получаем
$$
    a = e^{- \theta h},~ q= \sigma^2(1 - e^{-2 \theta h})
$$

Алгоритм фильтра Калмана.

Фильтр Калмана используется для оценки состояния $x_n$ системы на основе зашумленных измерений $y_n$. Алгоритм состоит из трех этапов: инициализация, прогнозирование и обновление.  

Инициализация \
На первом шаге задаются начальные условия для фильтра:  
$$
\hat{x}_0 = 0, \quad P_0 = \sigma^2,
$$
где:  
$\hat{x}_0$ — начальная оценка состояния, принимается равной $0$;  
$P_0$ — начальная ковариация ошибки состояния, равная $\sigma^2$, которая отражает дисперсию процесса.  

Прогнозирование \
На каждом шаге $n$ фильтр Калмана предсказывает состояние на основе модели системы:  
$$
\hat{x}_{n|n-1} = a \hat{x}_{n-1}, \quad P_{n|n-1} = a^2 P_{n-1} + q,
$$
где:  
$\hat{x}_{n|n-1}$ — прогнозируемое состояние системы в момент $n$;  
$P_{n|n-1}$ — прогнозируемая ковариация ошибки состояния.  

Обновление\
После получения нового измерения $y_n$, фильтр обновляет состояние и его ковариацию.  

1. Вычисление коэффициента Калмана:
$$
K_n = \frac{P_{n|n-1}}{P_{n|n-1} + r},
$$
где:  
$K_n$ — коэффициент Калмана, определяющий вес нового измерения $y_n$;  
$r$ — дисперсия (или параметр масштаба в случае распределения Коши) белого шума.  

2. Оценка состояния:
$$
\hat{x}_n = \hat{x}_{n|n-1} + K_n \big(y_n - \hat{x}_{n|n-1}\big),
$$
где:  
$y_n$ — текущее наблюдение;  
$y_n - \hat{x}_{n|n-1}$ — остаток, отклонение наблюдения от прогнозируемого состояния.  

3. Обновление ковариации ошибки:
$$
P_n = (1 - K_n) P_{n|n-1},
$$
где $P_n$ — обновленная ковариация ошибки состояния.  


Итоговый процесс фильтрации
\begin{enumerate}
  \item Инициализация: $\hat{x}_0$, $P_0$.  
  \item Прогноз: $\hat{x}_{n|n-1}$, $P_{n|n-1}$.  
  \item Обновление: $K_n$, $\hat{x}_n$, $P_n$.  
\end{enumerate}

Процесс повторяется для каждого нового измерения $y_n$.  

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{10_1.eps} 
    \caption{Фильтрация процесса Орнштейна-Уленбека от нормального шума}
\end{figure} 
Построение доверительных интервалов
\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{10_1(full).eps} 
    \caption{Доверительные интервалы}
\end{figure} 

Приведём ниже график, на котором будет отсутствовать шум, для наглядности доверительных интервалов.
\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{10_1(int).eps} 
    \caption{Доверительные интервалы}
\end{figure} 
\FloatBarrier


Теперь сделаем тоже самое, только для шума с распределением Коши. Для визуализации графиков будем брать шум, находящийся между $1$ и $99$ перцентилем, чтобы избавиться от сильно больших выбросов(более тяжёлые хвосты распределения). 

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{10_2.eps} 
    \caption{Фильтрация процесса Орнштейна-Уленбека от шума с распределением Коши}
\end{figure} 
Построение доверительных интервалов
\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{10_2(full).eps} 
    \caption{Доверительные интервалы}
\end{figure} 

Приведём ниже график, на котором будет отсутствовать шум, для наглядности доверительных интервалов.
\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{10_2(int).eps} 
    \caption{Доверительные интервалы}
\end{figure} 
\FloatBarrier




\section{Задание 11}
Построить двумерное пуассоновское поле, отвечающее сложному пуассоновскому процессу:
\begin{enumerate}
 \item Система массового обслуживания. Первая координата поля — время поступления заявки в СМО (распределенное равномерно), а вторая — время обслуживания заявки (распределение $\chi^2$ с десятью степенями свободы). 
  \item Система массового обслуживания с циклической интенсивностью $\lambda(1+\cos(t))$ и
единичными скачками. При помощи метода Льюиса и Шедлеара, свести задачу
моделирования неоднородного пуассоновского процесса к моделированию двумерного пуассоновского поля, где первая координата распределена равномерно,
а вторая имеет распределение Бернулли. 
  \item Работа страховой компании: первая координата — момент наступления стра-
хового случая (равномерное распределение), вторая — величина ущерба (распределение Парето). Поступление капитала считать линейным по времени со скоростью $c > 0$, начальный капитал $W > 0$.
\end{enumerate}

Чтобы определить Пуассоновское поле нам потребуется выполнения трёх пунктов.
\begin{enumerate}
  \item Хаотичность, как независимость мер попарно непересекающихся множеств.
  \item Пространственная однородность, как инвариатность относительно сдвига. \\
    - Это свойство может напоминать одно из альтернативных определений меры Лебега, с точностью домножения на константу.
  \item Ординарность, как требование на отсутствие "сосредоточенности в одной окрестности" точек.
\end{enumerate}

Работать с таким объектом будем с помощью считающих мер.
Считающая мера - мера, сосредоточенная не более, чем на счётном числе точек, обозначим множество этих точек $B$. Масса каждой точки будет равна единице. 
Определим $\nu(A) = |A \cap B|$ - количество точек во множестве $A$. \
Определим $\mu(A) = \mathbb{E} \nu(A)$ - среднее количество точек, попавнее в $A$. Одно из назнаний для такой величины - мера-интенсивность. \
Исходя из предъявленных требований, можем получить, что
$$
    \nu(A) \sim Pois(\mu(A)).
$$

\subsection{}

В данной задаче необходимо смоделировать систему массового обслуживания, где:
1. первая координата --- время поступления заявки в СМО, распределённое равномерно;
2. вторая координата --- время обслуживания заявки, распределённое по $\chi^2$ с $df=10$ степенями свободы.

В системе массового обслуживания будем работать на отрезке времени $[0;T]$. Введём случайную величину $\xi \sim Pois (\lambda T)$, где $\frac{1}{\lambda}$ - среднее время поступления заявки. Соответственно, $\xi$ - общее количество заявок.


Шаги моделирования с учётом того, что могут появляться очереди

1. Генерация времени поступления заявок:
   $$
   t_i \sim U(0, T), \, i = 1, \dots, N.
   $$
   И работать мы будем с вариационным рядом, чтобы отсортировать время поступления заявки.
   
2. Генерация времени обслуживания: \\
   - Для каждого $i$ генерируем время обслуживания заявки:
   $$
   s_i \sim \chi^2(k).
   $$

3. Учёт очередей: \\
   - Вычисляем момент завершения обслуживания каждой заявки:
   $$
   c_i = \max(t_i, c_{i-1}) + s_i, \quad c_0 = s_0.
   $$

Для оценки качества работы системы будем считать количество заявок в очереди в момент времени $t$:
$$
   N(t) = \sum^{\xi}_{i=1} t_{(i)} \leqslant t \leqslant c_i.
$$
Сколько заявок поступило, но ещё не было обработано.

Среднее время обработки заявки - это математическое ожидание $s_i$, оно равно количеству степеней свободы в системе (В данном случае $df=10$). \\
Таким образом, у нас могут быть 3 режима работы исходя из соотношения $\frac{1}{\lambda} \vee 10$, перепишем иначе $\lambda \vee \frac{1}{10}$:
\begin{enumerate}
 \item Если $\lambda$  меньше, то заявки будут поступать дольше, чем обрабатываться, следовательно система будет справляться.
 \item Если $\lambda = \frac{1}{10}$, то система будет находиться в так называемом "равновесии", где примерно одинаково заявки будут набираться и обрабатываться.
 \item Если $\lambda$  больше, то заявки будут поступать быстрее, чем обрабатываться, следовательно система не будет справляться, и очередь будет набираться.
\end{enumerate}  


\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{11_1(1).eps} 
    \caption{Система справляется}
\end{figure} 

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{11_1(2).eps} 
    \caption{Система в равновесии}
\end{figure} 

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{11_1(3).eps} 
    \caption{Система не справляется}
\end{figure} 
\FloatBarrier

\subsection{}
Нам предложена система массового обслуживания с переменной интенсивностью поступления заявок, описываемой функцией $\lambda(t)=\lambda(1+\cos(t))$.\
Это означает, что интенсивность потока заявок периодически меняется с течением времени.


Двумерное пуассоновское поле:
Мы будем генерировать точки на плоскоти $(t, u)$, где $t$ - время, а $u$ - случайная величина, распределенная по Бернулли.\
Если значение $u$ в точке $(t, u)$ равно $1$, то в момент времени $t$ происходит событие (поступление заявки).

Математическая постановка и алгоритм
1. Определение интенсивности:

$\lambda(t)=\lambda(1+\cos(t))$ - интенсивность поступления заявок в момент времени $t$. 
2. Преобразование к однородному пуассоновскому процессу:

Введем функцию 
$$
\Lambda(t) = \int_0^t \lambda(s) ds = \lambda(t + \sin(t)),
$$
которая определяет среднее число событий на интервале $[0; t]$. С помощью данной величины определим $\mu_0 = \Lambda(T) - \Lambda(0) = \Lambda(T)$. \\
Таким образом, мы получили параметр пуассоновского процесса для генерации $\xi \sim Pois(\mu_0)$. \\
Несложно получить, что $\lambda_{max} = 2 \lambda$ (максимальное значение $\lambda(t)$).\
Сгенерируем точки на плоскоти $(t, u)$, где $t$ распределено равномерно на отрезке $[0; T]$, а $u$ распределено по Бернулли с вероятностью успеха $p(t)= \frac{\lambda(t)}{\lambda_{max}}$. \\
3. Отбор событий:

Оставим только те точки $(t, u)$, для которых $u = 1$. Эти точки соответствуют моментам поступления заявок в исходном неоднородном процессе.\\
4. Моделирование времени обслуживания:

Для каждой заявки сгенерируем время обслуживания из распределения $\chi^2$ с 10 степенями свободы. \\
5. Моделирование очереди:

Генерация точек на плоскости:
$$
(t_i, u_i) \sim U(0, T) \times Bern\left(\frac{\lambda(t_i)}{\lambda_{max}}\right), \quad i = 1, \dots, \xi
$$
где $\xi$ - общее число сгенерированных точек.

Отбор событий:
$$
\text{Если } u_i = 1, \text{ то } t_i \text{ - момент поступления заявки.}
$$

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{11_2.eps} 
    \caption{Система не справляется}
\end{figure} 
\FloatBarrier

\subsection{}

Работа страховой компании: первая координата — момент наступления страхового случая (равномерное распределение), вторая — величина ущерба (распределение Парето). Поступление капитала считать линейным по времени со скоростью $c > 0$, начальный капитал $W > 0$.


В данной системе также будем работать на отрезке времени $[0;T]$. Введём случайную величину $\xi \sim Pois (\lambda T)$, где $\frac{1}{\lambda}$ - среднее время наступления страхового случая.\
Соответственно, $\xi$ - общее количество случаев.

1. Генерация времени наступления случаев:
   $$
   t_i \sim U(0, T), \, i = 1, \dots, N.
   $$
   И работать мы будем с вариационным рядом, чтобы отсортировать время "краха".
   
2. Генерация момента:
   - Для каждого $i$ генерируем величину ущерба:
   $$
   s_i \sim P(k, x_m),
   $$
   $x_m$ - коэффициент масштаба

   
Выпишем зависимость капитала от времени
$$
    K(t) = W + ct - \sum_{k=1}^{\xi}s_i
$$
Будем считать, что капитал неотрицательный, поэтому если получается иное, то заключаем, что компания банкротится.

Случайная величина с распределением Парето имеет бесконечное математическое ожидание при $k \leqslant 1$, поэтому в таком режиме компания рано или поздно объявит своё банкротство. 

Выпишем функцию 
Распределение Парето и его функция распределения
Функция распределения F(x) для распределения Парето с параметрами (a, b) имеет вид:
$$
F(x) = 1 - (a/x)^b,  \text{ для } x >= a
$$
Обратная функция к F(x) будет:
$$
  x = F^{-1}(u) = a / (1-u)^{1/b}
$$

Пусть $X$ – случайная величина, имеющая распределение Парето с параметрами $(a, b)$. 
Функция распределения $X$ имеет вид:
$$
F(x) = 1 - \left(\frac{x_m}{x}\right)^k, \quad x \geqslant x_m.
$$
Обратная функция к $F(x)$:
$$
x = F^{-1}(u) = \frac{x_m}{(1-u)^{1/k}},
$$
где $u$ – случайное число, равномерно распределенное на отрезке $[0, 1]$. Однако, в данном случае $u$ и $1-u$ принадлежат одному распределению, можем переписать в виде
$$
x = F^{-1}(u) = \frac{x_m}{(u)^{1/k}}.
$$

Можем выписать ожидаемый капитал в каждый момент времени
$$
    \mathbb{E}K(t) = W + ct - \lambda t \frac{k x_m}{k-1}.
$$
При $ c - \lambda \frac{k x_m}{k - 1} > 0$, мы ожидаем, что капитал в среднем растёт. В противном случае, убывать, и через какое-то время разоряться.

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{11_3.eps} 
    \caption{}
\end{figure} 
\FloatBarrier

  
\begin{thebibliography}{99}
  \bibitem{1} Смирнов С.Н. Курс лекций “Стохастический анализ и моделирование”, 2024. 
  \bibitem{2}  Пономаренко Л.С. Курс лекций “Теория вероятностей и математическая статистика”, 2022-2023.
  \bibitem{3} Ширяев А.Н. Вероятность. – в 2-х кн., 3-е изд. – М. МЦНМО, 2004.
  \bibitem{4} Соболь И.М. Численные методы Монте-Карло. — Наука, 1973
  \bibitem{5} Тихонов А.Н., Самарский А.А. Уравнения математической физики. — МГУ.
  \bibitem{6} P.A.W. Lewis, G.S. Shedler, “Simulation of nonhomogeneous poisson processes by thinning”, IBM Research Laboratory, San Jose, California, 1979.
\end{thebibliography}

\end{document}
